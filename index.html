<!DOCTYPE HTML>

<html>
	<head>
		<title>My Personal portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo">My personal data engineering portfolio</a>
									<ul class="icons">
										<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
									</ul>
								</header>
							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<header>
											<h1>Hi, Iâ€™m ADENIJI Mujeeb <br /></h1>
											<p> A Data Engineer </p>
										</header>
										<p> I have been in the data space for quite some now. I have previously worked as a data analyst and as an analytics engineer
											before I transitioned into data engineering. I am interested in knowing how things work on the engineering side of data, thus
											the reason for my transitioning. I have about four years of work experience as a data engineer and I am willing to use the
											knowledge and expertise gained from my previous experience to help companies build and maintain their data pipelines. I am currently open to a new role as a data engineer.<br />
											Below are the architectures of some of the data pipeline that I have built personally, this does not contain any company's project or collaborations made.
										</p>
									</div>
									
								</section>
							
								<section>
									<header class="major">
										<h2>Technical Skills</h2>
									</header>
									<ul class="icons">
										<li>Python,</li>
										<li>PostgreSQL,</li>
										<li>MySQL,</li>
										<li>GCP,</li>
										<li>AWS,</li>
										<li>Snowflake,</li>
										<li>Docker,</li>
										<li>Kubernetes,</li>
										<li>Terraform,</li>
										<li>Git,</li>
										<li>Apache Airflow,</li>
										<li>Apache Spark,</li>
										<li>DBT,</li>
										<li>Apache Kafka,</li>
										<li>Jenkins,</li>
										<li>Github Action,</li>
										<li>Power BI,</li>
										<li>Google data studio,</li>
										<li>Qlik,</li>
										<li>Confluence,</li>
										<li>JIRA</li>
									</ul>
								</section>
								<section>
									<header class="major">
										<h2>Previous works (Personal)</h2>
									</header>
									<div class="posts">
										<article>
											<a href="#" class="image"><img src="images/pic01.jpg" alt="" /></a>
											<h3>ETL - Data ingestion from dynamoDB to Redshift</h3>
											<p> In this architecture, the assumed scenerio was a gaming data and the following tools/services were used: <br>
											<ul>
											<li>Lambda Function</li>
											<li>Amason S3</li>
											<li>Athena</li>
											<li>Quicksight</li>
											<li>AWS Glue Databrew</li>
											<li>SNS</li>
											<li>AWS Glue ETL Job</li>
											<li>Amazon Redshift</li>
											</ul>
												The data was ingested from DynamoDB to Redhsift(batch process). A lambda function
												was invoked on Amazon S3 bucket so that whenever there is a new update, insert or delete in the
												dynamoDB, the lambda function will trigger and replicate the data into S3 raw data bucket.
												Then, another Lambda function was invoked on the S3 raw data bucket to run AWS step function to
												orchestrate the data process and transformation. Glue Databrew is used for profiling and checking
												for data quality, the cleaned data will be stored on another S3 bucket. SNS will be used for
												notification in case there is any failure in the data quality checks.
												The AWS Glue Job is used for transforming the data to meet up with the end user use-case and the
												data will be finally ingested into Redshift (data warehouse). All this process are run on Step Function
												in other to orchestrate and monitor our workflow.</p>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic04.jpg" alt="" /></a>
											<h3>Streaming data from neo4j to Redshift</h3>
											<p>In this architecture, the assumed scenerio was a gaming data (streaming) and the following tools/services were used:
											<ul>
											<li>Lambda Function</li> 
											<li>Amason Kinesis Firehose</li>
											<li>Amason S3</li>
											<li>Eventbridge</li>
											<li>Secrets Manager</li>
											<li>Amazon Kinesis Data Analytics</li>
											<li>AWS Glue Databrew</li>
											<li>SNS</li>
											<li>Amazon Redshift</li>
											</ul>
											The data was streamed using Amazon Kinesis Data Firehouse from the
												assumed neo4j database that is running on Elastic Kubernete Service(EKS). A lambda function was
												invoked on kinesis data firehose to deliver the streamed data into S3 raw bucket.
												From S3 raw bucket, lambda function and eventbridge was used to trigger step function which will
												serve as an orchestration workflow to run Kinesis Data Analytics to perform analysis, AWS Glue
												Databrew was used to perform data profiling and data quality checks on the S3 curated bucket and
												when the DQ rules(for data quality checks) and data profiling successfully passes, the cleaned data will be sent to the
												Amazon Redhift(data warehouse) and if otherwise a notification will be sent to the data engineer to
												check for errors or perform data enrichment.</p>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic02.jpg" alt="" /></a>
											<h3>ETL pipeline using on-prem database</h3>
											<p>The purpose of this project is to perform data enrichment so that the total profit
												made on all car sales can be calculated. This diagram contains an ETL pipeline for performing data enrichment, 
												the data source which are the CSV files. Surrogate keys were introduced into the data as the data was loaded into database 
												in other to have unique records. I use docker, airflow and postgres. Docker was used to dockerize airflow and postgres,
												Airflow for orchestrating and scheduling the jobs(there were seven tasks all together).
												I used postgres as the database and docker for containerising the application.
												The code can be found here: <a href="https://github.com/AdenijiMujeeb/postgres-airflow">Link to repo</a>
												</p>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic03.jpg" alt="" /></a>
											<h3>ETL pipeline using google cloud platform</h3>
											<p>This is a proposed architecture for deploying figure 3 into cloud as the project was tested on on-prem database using pgAdmin</p>
										</article>
									</div>
								</section>
						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
								
							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="#">adenijimujeeb@gmail.com</a></li>
										<li class="icon solid fa-phone">+447926534581</li>
										<li class="icon solid fa-home">27 Pendrill Street. Hull, United Kingdom</li>
									</ul>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Untitled. All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
